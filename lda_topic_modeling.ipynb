{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel, LdaMulticore\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, LancasterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer1 = SnowballStemmer('english')\n",
    "stemmer2 = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "code largely appropriated from these websites\n",
    "visualizations:\n",
    "https://jeriwieringa.com/2017/06/21/Calculating-and-Visualizing-Topic-Significance-over-Time-Part-1/\n",
    "\n",
    "topics modeling:\n",
    "https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "\n",
    "raising errors:\n",
    "https://docs.python.org/3/tutorial/errors.html\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error(Exception):\n",
    "    '''Base class for exceptions in this module.'''\n",
    "    pass\n",
    "\n",
    "class InputError(Error):\n",
    "    '''Exception raised for errors in the input.\n",
    "    \n",
    "    Attributes:\n",
    "        expression -- input expression in which the error occurred\n",
    "        message -- explanation of the error\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, expression, message):\n",
    "        self.expression = expression\n",
    "        self.message = message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicModel():\n",
    "    \n",
    "    def __init__(self, filename, sort_by = 'year', num_topics=15):\n",
    "        \n",
    "        self.sort_by = sort_by\n",
    "        \n",
    "        '''\n",
    "        importing the data\n",
    "        our import process is different for .csv and .xlsx documents.\n",
    "        the goal is to end with the data columns we want and the search terms from the patent database.\n",
    "        the steps to accomplish this differ for .csv and .xlsx documents.\n",
    "        .csv steps\n",
    "            -remove first row of export (a descriptor, not a header)\n",
    "            -extract search terms from that descriptor\n",
    "        .xlsx steps\n",
    "            -import first line of .csv document, containing search terms\n",
    "            -extract search terms from that data\n",
    "            -import entire .csv file, skipping the first two lines\n",
    "        '''\n",
    "        if filename.endswith('.csv'):\n",
    "            searchterm_df = pd.read_csv(filename, nrows = 1)\n",
    "            self.searchterms = self.get_searchterms_csv(searchterm_df)\n",
    "            \n",
    "            my_data = []\n",
    "            with open('Downloads\\patentpulltestfacialrecognition071719.csv', newline='', encoding='utf8') as f:\n",
    "                reader = csv.reader(f)\n",
    "                for row in reader:\n",
    "                    my_data.append(row)\n",
    "            self.data = pd.DataFrame(my_data[2:])\n",
    "            new_header = self.data.iloc[0] # grab the first row for the header\n",
    "            self.data = self.data[1:] # take the data lwithout the header row\n",
    "            self.data.columns = new_header # set the header row as the df header\n",
    "        \n",
    "        elif filename.endswith('.xlsx'):\n",
    "            self.data = pd.read_excel(filename, skiprows=1)\n",
    "            \n",
    "            searchterm_df = pd.read_excel(filename, nrows=1)\n",
    "            self.searchterms = self.get_searchterms_xlsx(searchterm_df)\n",
    "        \n",
    "        else:\n",
    "            raise InputError(filename, 'filename should end in .csv or .xlsx')\n",
    "        \n",
    "        '''\n",
    "        after these steps, the steps for both file types are the same\n",
    "            -we stem our search terms\n",
    "            -we select our relevant columns: abstracts, source info, and application date.\n",
    "            -the 'Assignee - Original - Country/Region' is not useful so we pull out country codes from it.\n",
    "            -depending on the parameters, we trim 'Application Date' down to Year-Month, just Year, or leave it as Year-Month-Day.\n",
    "        '''\n",
    "        \n",
    "        # now let's stem the search terms so they align with our stemmed data\n",
    "        self.cleaned_terms = []\n",
    "        for term in self.searchterms:\n",
    "            self.cleaned_terms.append(self.stem2(term))\n",
    "        \n",
    "        # select relevant columns\n",
    "        # we are interested in Abstract DWPI (our main focus of analysis),\n",
    "        # Assignee - Original - Country/Region (indicates the source country),\n",
    "        # and Application Date.\n",
    "        # We use Abstract DWPI instead of just DWPI because it is generally cleaner and more informative.\n",
    "        self.documents = self.data[['Abstract - DWPI', 'Assignee - Original - Country/Region', 'Application Date']]\n",
    "        \n",
    "        # Assignee - Original - Country/Region consists of company names and country codes.\n",
    "        # We only want the country codes.\n",
    "        countries = []\n",
    "        for i in self.documents['Assignee - Original - Country/Region']:\n",
    "            if len(str(i).split(',')[1]) > 1: # to make sure our list index is not out of range in the next line\n",
    "                if len(str(i).split(',')[1]) > 1: # same thing\n",
    "                    country = str(i).split(',')[1][:2] # the two characters following the comma are most consistently country codes\n",
    "                    countries.append(country)\n",
    "                else:\n",
    "                    country.append('n/a')\n",
    "            else:\n",
    "                countries.append('n/a')\n",
    "        self.documents['Country'] = countries\n",
    "        self.documents.drop(['Assignee - Original - Country/Region'], axis = 1)\n",
    "        # attribute of unique country codes for user convenience\n",
    "        self.unique_countries = self.documents['Country'].unique()\n",
    "        \n",
    "        # Application Date's format is YYYY-MM-DD but this may be too granular to see trends in the data.\n",
    "        # If the user specifies sort_by = 'Month', we only care about YYYY-MM so we cut off the -DD.\n",
    "        old_height, _ = self.documents.shape\n",
    "        if self.sort_by == 'month':\n",
    "            self.documents['Application Date'] = self.documents['Application Date'].apply(lambda x: str(x)[:7])\n",
    "        # If the user specifies sort_by 'Year', we only care about YYYY so we cut off the -MM-DD.\n",
    "        elif self.sort_by == 'year':\n",
    "                self.documents['Application Date'] = self.documents['Application Date'].apply(lambda x: str(x)[:5])\n",
    "        # If the user specifies sort_by = 'Day', we don't need to change anything.\n",
    "        elif self.sort_by == 'day':\n",
    "            pass\n",
    "        else:\n",
    "            raise InputError(self.sort_by, 'sort_by must be set as \\'year\\', \\'month\\', or \\'day\\'.')\n",
    "        # to_datetime converts the date string to a datetime object, telling the dataframe that 2015-04 comes after 2015-03\n",
    "        self.documents['Application Date'] = pd.to_datetime(self.documents['Application Date'], errors='coerce')\n",
    "        # to_datetime only accepts dates from around 1600 to 2200. errors='coerce' makes dates outside this range in NaT values\n",
    "        \n",
    "        # we remove the bad values here\n",
    "        self.documents = self.documents.dropna(subset=['Application Date'])\n",
    "        new_height, _ = self.documents.shape\n",
    "        # to make sure we don't remove too many rows (which would indicate a larger problem), we print how many rows we removed\n",
    "        height_change = old_height - new_height\n",
    "        print('We removed {0} rows due to bad dates'.format(height_change))\n",
    "\n",
    "        # indexing the documents, creating a simple ID\n",
    "        length, width = self.documents.shape\n",
    "        index_column = list(range(length)) # list from 0 to length of dataframe (i.e. number of abstracts)\n",
    "        self.documents.insert(0, 'index_pos', index_column) # adds this new ID column to the dataframe\n",
    "        \n",
    "        # dropping documents with no abstract\n",
    "        old_height, _ self.documents.shape\n",
    "        self.documents['Abstract - DWPI'].reaplace('', np.nan, inplace=True)\n",
    "        self.clean_documents = self.documents.dropna(subset=['Abstract - DWPI'])\n",
    "        new_height, _ = self.clean_documents.shape\n",
    "        height_change = old_height - new_height\n",
    "        print('We removed {0} rows due to missing abstracts'.format(height_change))\n",
    "        \n",
    "        # preprocess the data\n",
    "        processed_docs = self.clean_documents['Abstract - DWPI'].map(self.preprocess)\n",
    "        # indexing the words in the corpus\n",
    "        dictionary = Dictionary(processed_docs)\n",
    "        # removing extremely rare and common words\n",
    "        dictionary.filter_extremes(no_below=15, no_above=0.4, keep_n=100000)\n",
    "        # removes words in fewer than 15 documents or more than 40% of the corpus. Only keeps the first n most frequent words.\n",
    "        \n",
    "        # create the model\n",
    "        # converts documents to bag of words (bow). bow consists of token IDs and their frequency counts for every document.\n",
    "        bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "        # tf-idf is a measure of word importance, just like frequency count could be considered a measure of importance.\n",
    "        # However, tf-idf assumes that words that appear in a high percentage of documents are less important, but if they appear frequently in one document, they are more important.\n",
    "        tfidf = TfidfModel(bow_corpus)\n",
    "        corpus_tfidf = tfidf[bow_corpus]\n",
    "        \n",
    "        # our model using Gensim\n",
    "        self.lda_model = LdaMulticore(corpus_tfidf, num_topics=num_topics, id2word=dictionary, passes=10, workers=2)\n",
    "        \n",
    "        # now we start building the dataframe we will visualize\n",
    "        # build a key of topics and ids\n",
    "        topic_words = []\n",
    "        topic_id = []\n",
    "        # the topics come in a list format, so this for loop breaks down the list into a string\n",
    "        for idx, topic in self.lda_model.print_topics(-1):\n",
    "            words = topic.split('\"\"') # splits\n",
    "            words = words[1::2]\n",
    "            topic_words.append(words)\n",
    "            topic_id.append(idx)\n",
    "        self.topic_labels = pd.DataFrame(list(zip(topic_id, topic_words)), columns = ['topic_id', 'topic_words'])\n",
    "        # make the list of topic words into a string\n",
    "        self.topic_labels['topic_words'] = self.topic_labels['topic_words'].apply(lambda x: str(','.join(x)))\n",
    "        \n",
    "        # build a dataframe of how the topics appear in each document\n",
    "        index_pos = []\n",
    "        topic_id = []\n",
    "        topic_weight = []\n",
    "        count = 0\n",
    "        for topics in self.lda_model.get_document_topics(bow_corpus):\n",
    "            for topic in topics:\n",
    "                index_pos.append(count)\n",
    "                top_id, top_weight = topic\n",
    "                topic_id.append(top_id)\n",
    "                topic_weight.append(top_weight)\n",
    "            count += 1\n",
    "        topic_dtm = pd.DataFrame(list(zip(index_pos, topic_id, topic_weight)), columns = ['index_pos', 'topic_id', 'topic_weight'])\n",
    "        # Normalizing the topic weight dataframe\n",
    "        # Reorient from long to wide\n",
    "        dtm = topic_dtm.pivot(index='index_pos', columns='topic_id', values='topic_weight').fillna(0)\n",
    "        # Divide each value in a row by the sum of the row to normalize the values\n",
    "        dtm = (dtm.T/dtm.sum(axis=1)).T\n",
    "        # Shift back to a long dataframe\n",
    "        dt_norm = dtm.stack().reset_index()\n",
    "        dt_norm.columns = ['index_pos', 'topic_id', 'norm_topic_weight']\n",
    "        \n",
    "        # merging dataframes\n",
    "        topics_expanded = dt_norm.merge(self.topic_labels, on='topic_id')\n",
    "        df = topics_expanded.merge(self.clean_documents, on='index_pos', how='left')\n",
    "        \n",
    "        # isolating the top topics\n",
    "        topics = list(self.topic_labels['topic_id'])\n",
    "        scores = []\n",
    "        for topic_id in topics:\n",
    "            # for every topic id, we record its scores\n",
    "            scores.append(np.mean(df[(df['topic_id'] == topic_id)]['norm_topic_weight']))\n",
    "        topic_scores = pd.DataFrame(list(zip(topics, scores)), columns=['topic_id', 'scores'])\n",
    "        self.sorted_topic_scores = topic_scores.sort_values(by=['scores'], ascending = False)\n",
    "        # sorting the data by date\n",
    "        self.df = df.sort_values(by=['Application Date'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    def stem1(self, text):\n",
    "        '''\n",
    "        Snowball stemmer used in preprocess()\n",
    "        Weaker stemmer than Lancaster. Used for stemming corpus.\n",
    "        '''\n",
    "        return stemmer1.stem(text)\n",
    "    \n",
    "    def stem2(self, text):\n",
    "        '''\n",
    "        Lancaster stemmer used in preprocess()\n",
    "        More aggressive stemmer than Snowball, used to insure search terms do not appear in corpus.\n",
    "        '''\n",
    "        return stemmer2.stem(text)\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        '''\n",
    "        Preprocessing of data for modeling.\n",
    "        Tokenizes, removes stopwords and short words, removes puncuation, makes everything lowercase, stems the words, and removes words related to original search terms.\n",
    "        '''\n",
    "        result = []\n",
    "        for token in simple_preprocess(text):\n",
    "            if token not in STOPWORDS and len(token) > 3:\n",
    "                stemmed2 = self.stem2(token)\n",
    "                if stemmed2 not in self.cleaned_terms: # we do not care about our search terms\n",
    "                    stemmed1 = self.stem1(token)\n",
    "                    result.append(stemmed1)\n",
    "        return result\n",
    "    \n",
    "    def get_searchterms_xlsx(self, data):\n",
    "        '''\n",
    "        returns the original search terms from the patent database\n",
    "        '''\n",
    "        terms = []\n",
    "        searchterms = ['none']\n",
    "        for item in list(data): # This patent database includes search terms in their top line\n",
    "            if 'Search results' in str(item):\n",
    "                searchterms = item\n",
    "                break\n",
    "        if searchterms == ['none']:\n",
    "            return searchterms\n",
    "        words = searchterms.split('\"\"') # the search terms are in quotations\n",
    "        words = words[1::2] # after separating by delimiters, we want only every otehr piece\n",
    "        for word in words:\n",
    "            individ = words.split(' ') # if two terms were searched together, we want to separate them\n",
    "            terms = terms + individ\n",
    "        return terms\n",
    "    \n",
    "    def get_searchterms_csv(self, data):\n",
    "        '''\n",
    "        returns the original search terms from the patent database\n",
    "        '''\n",
    "        str_data = str(data)\n",
    "        words = str_data.split('\"\"') # the search terms are in quotations\n",
    "        words = words[1::2] # after separating by delimiters, we want only every otehr piece\n",
    "        terms = []\n",
    "        for word in words:\n",
    "            individ = word.split(' ') # if two terms were searched together, we want to separate them\n",
    "            terms = terms + individ\n",
    "        return terms\n",
    "    \n",
    "    def line_graph(self, start = None, end = None, country = None, top = 5):\n",
    "        '''\n",
    "        This method is the main visualization tool. It visualizes the prominence of the top topics over time based on the norm_topic_weight variable.\n",
    "        For one year, the y-value of a topic represents that topic's average topic weight for that year, including in abstracts in which it has a weight of zero.\n",
    "        Parameters\n",
    "            -start: optional start date for visualization\n",
    "            -end: optional end date for visualization\n",
    "            -country: optional country (source of patents) to focus on\n",
    "            -top: number of top topics to be visualized\n",
    "        '''\n",
    "        top_topics = list(self.sorted_topic_scores['topic_id'][0:top]) # gets the top prevalent topics\n",
    "        # specify order\n",
    "        sorted_order = []\n",
    "        graph_df = pd.DataFrame()\n",
    "        for i in top_topics: # we only want to graph the top topics\n",
    "            sorted_order.append(self.topic_labels['topics words'][i])\n",
    "            current_df = self.df[(self.df['topic_id'] -- i)]\n",
    "            graph_df = pd.concat([graph_df, current_df])\n",
    "        \n",
    "        # years, if specified\n",
    "        if start:\n",
    "            start = datetime(start, 1, 1)\n",
    "            graph_df = graph_df[(graph_df['Application Date'] >= start)]\n",
    "        if end:\n",
    "            end = datetime(end, 1, 1)\n",
    "            graph_df = graph_df[(graph_df['Application Date'] < end)]\n",
    "        # country, if specified\n",
    "        if country:\n",
    "            graph_df = graph_df[(graph_df['Country'] == country)]\n",
    "        else:\n",
    "            country = 'all'\n",
    "        \n",
    "        # the following step insures that the x-axis tick labels are legible and show only the specified date\n",
    "        unique_dates = graph_df['Application Date'].unique()\n",
    "        x_values = []\n",
    "        if self.sort_by == 'year':\n",
    "            for date in unique_dates:\n",
    "                year = str(date)[:4]\n",
    "                x_values.append(year)\n",
    "        elif self.sort_by == 'month':\n",
    "            for date in unique_dates:\n",
    "                month = str(date)[:7]\n",
    "                x_values.append(month)\n",
    "        else: # self.sort_by == 'day'\n",
    "            for date in unique_dates:\n",
    "                day = str(date)[:10]\n",
    "                x_values.append(day)\n",
    "        \n",
    "        # create pointplot\n",
    "        p = sns.catplot(x='Application Date', y='norm_topic_weight', kind='point', hue_order=sorted_order, hue='topic_words',\n",
    "                        col=None, col_wrap=None, col_order=sorted_order, height=5, aspect=1.5, data=graph_df, ci=None)\n",
    "        p.set(xticklabels=x_values)\n",
    "        for axis in p.axes.flat:\n",
    "            for item in axis.get_xticklabels():\n",
    "                item.set_rotation(90) # rotates the x-axis tick labels\n",
    "        p.fig.subplots_adjust(top=0.9)\n",
    "        p.fig.suptitle(t='Average Normalized Topic Weights. Search terms: {0}. Country: {1}'.format(self.searchterms, country), fontsize=16)\n",
    "        return p\n",
    "    \n",
    "    def get_abstracts(self, topic_id, start = None, end = None, country = None, top = 5):\n",
    "        '''\n",
    "        This method pulls the ten abstracts that are most made up of the chosen topic.\n",
    "        These abstracts should reflect the topic the best.\n",
    "        Parameters\n",
    "            -start: optional start date\n",
    "            -end: optional end date\n",
    "            -country: optional country (source of patents)\n",
    "            -top: number of top topics\n",
    "        '''\n",
    "        abstract_df = self.df[(self.df['topic_id'] == topic_id)]\n",
    "        abstract_df = abstract_df.sort_values(by = ['norm_topic_weight'], ascending = False)\n",
    "        # years, if specified\n",
    "        if start:\n",
    "            start = datetime(start, 1, 1)\n",
    "            abstract_df = abstract_df[(abstract_df['Application Date'] >= start)]\n",
    "        if end:\n",
    "            end = datetime(end, 1, 1)\n",
    "            abstract_df = abstract_df[(abstract_df['Application Date'] < end)]\n",
    "        # country, if specified\n",
    "        if country:\n",
    "            abstract_df = abstract_df[(abstract_df['Country'] == country)]\n",
    "        for i in range(top):\n",
    "            print(abstract_df['Abstract - DWPI'].iloc[i])\n",
    "        return abstract_df[:top]\n",
    "    \n",
    "    def get_topicdata(self, start = None, end = None, country = None):\n",
    "        '''\n",
    "        This method returns a dataframe of the same data as is graphed in the line_graph().\n",
    "        In other words, for every topic ID, the dataframe shows for every year that topic ID's average topic weight.\n",
    "        Parameters\n",
    "            -start: optional start date\n",
    "            -end: optional end date\n",
    "            -country: optional country (source of patents)\n",
    "            -top: number of top topics\n",
    "        '''\n",
    "        topic_df = self.df\n",
    "        if country:\n",
    "            topic_df = topic_df[(topic_df['Country'] == country)]\n",
    "        dates = topic_df['Application Date'].unique()\n",
    "        topic_data = pd.DataFrame()\n",
    "        topic_data['Date'] = dates\n",
    "        for topic_id in self.topic_labels['topic_id']:\n",
    "            results = []\n",
    "            cur_df = topic_df[(topic_df['topic_id'] == topic_id)]\n",
    "            for date in topic_data['Date']:\n",
    "                date_df = cur_df[(cur_df['Application Date'] == date)]\n",
    "                result = date_df['norm_topic_weight'].mean()\n",
    "                results.append(result)\n",
    "            topic_data['{0}'.format(topic_id)] = results\n",
    "        if start:\n",
    "            start = datetime(start, 1, 1)\n",
    "            topic_data = topic_data[(topic_data['Date'] >= start)]\n",
    "        if end:\n",
    "            end = datetime(end, 1, 1)\n",
    "            topic_data = topic_data[(topic_data['Date'] < end)]\n",
    "        return topic_data\n",
    "    \n",
    "    def count_graph(self, start = None, end = None, country = None, output = 'graph'):\n",
    "        '''\n",
    "        This method returns either a  graph or dataframe of how many patents there were in each year.\n",
    "        The purpose of this method is to prevent analysts from putting too much weight on outliers, since the prevalence of a topic in one year may be due to the scarty of documents in that year.\n",
    "        Parameters\n",
    "            -start: optional start date\n",
    "            -end: optional end date\n",
    "            -country: optional country (source of patents)\n",
    "            -output: 'graph' or 'table'\n",
    "        '''\n",
    "        topic_data = self.clean_documents\n",
    "        if country:\n",
    "            topic_data = topic_data[(topic_data['Country'] == country)]\n",
    "        if start:\n",
    "            start = datetime(start, 1, 1)\n",
    "            topic_data = topic_data[(topic_data['Application Date'] >= start)]\n",
    "        if end:\n",
    "            end = datetime(end, 1, 1)\n",
    "            topic_data = topic_data[(topic_data['Application Date'] < end)]\n",
    "        \n",
    "        dates = topic_data['Application Date'].unique()\n",
    "        count_df = pd.DataFrame()\n",
    "        count_df['Date'] = dates\n",
    "        counts = []\n",
    "        for date in count_df['Date']:\n",
    "            current_df = topic_data[(topic_data['Application Date'] == date)]\n",
    "            count, _ = current_df.shape\n",
    "            counts.append(count)\n",
    "        count_df['Counts'] = counts\n",
    "        \n",
    "        count_df = count_df.sort_values(by=['Date'])\n",
    "        \n",
    "        if output == 'graph':\n",
    "            # the following step insures that the x-axis tick labels are legible and show only the specified date\n",
    "            unique_values = count_df['Date']\n",
    "            x_values = []\n",
    "            if self.sort_by == 'year':\n",
    "                for date in unique_dates:\n",
    "                    year = str(date)[:4]\n",
    "                    x_values.append(int(year))\n",
    "            elif self.sort_by == 'month':\n",
    "                for date in unique_dates:\n",
    "                    month = str(date)[:7]\n",
    "                    x_values.append(month)\n",
    "            else: # self.sort_by == 'day'\n",
    "                for date in unique_dates:\n",
    "                    day = str(date)[:10]\n",
    "                    x_values.append(day)\n",
    "            ax = sns.lineplot(x='Date', y='Count', data=count_df)\n",
    "            # ax.set(xticklables=x_values)\n",
    "            for item in ax.get_ticklabels():\n",
    "                item.set_rotation(90) # rotates the x-axis tick labels\n",
    "            ax.set_title(label='Count of Documents by Year. Search terms: {0}. Country: {1}'.format(self.searchterms, country))\n",
    "            return ax\n",
    "        elif output == 'table':\n",
    "            return count_df\n",
    "        else:\n",
    "            raise InputError(output, 'output must be \\'graph\\' or \\'table\\'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = TopicModel('<input_data_set.csv>', sort_by = 'year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_model.searchterms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_model.cleaned_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_model.unique_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.clean_documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.topic_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.sorted_topic_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.line_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.line_graph(start=2009, country='US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.line_graph(start=2009, country='CN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.get_abstracts(topic_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_data = my_model.get_topicdata()\n",
    "topic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.count_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_table = my_model.count_graph(output='table')\n",
    "count_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "08/06/19\n",
    "\n",
    "Creator: Soren Gran\n",
    "\n",
    "My website: sorengran.com\n",
    "\n",
    "Please contact me there if you have any questions or concerns about this code.\n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
